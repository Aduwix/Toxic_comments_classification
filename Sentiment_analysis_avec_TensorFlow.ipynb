{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_analysis_avec_TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aduwix/Toxic_comments_classification/blob/main/Sentiment_analysis_avec_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5bUWT9PbE2y"
      },
      "source": [
        "# Analyse des sentiments à l'aide de l'apprentissage profond"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bohe_56hbQ9q"
      },
      "source": [
        "Dans ce projet, nous allons utiliser un ensemble de [données Kaggle](https://www.kaggle.com/kazanova/sentiment140) pour l'analyse des sentiments. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_tkEBNpbcnK"
      },
      "source": [
        "# Importation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vyp3YYYmbn8k"
      },
      "source": [
        "Ajoutez un raccourci de ce dossier à votre google drive :\n",
        "\n",
        "https://drive.google.com/drive/folders/1uj-BnUzSHJOHuojQ9q8q53DN0EGiPpcl?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFjOtn0obEl1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a227e9ee-dcd6-4e2e-d58e-7bfa1425f7da"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQZedNAMby-0"
      },
      "source": [
        "# Importation des packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-8Q64rmaR0L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bde34990-09a9-46c1-91fb-1f4dcb5e6420"
      },
      "source": [
        "from time import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import Regex to clean up tweets\n",
        "import re\n",
        "\n",
        "import nltk, string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# Get Reviews\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Get Tweets\n",
        "import httplib2\n",
        "import requests\n",
        "import urllib3\n",
        "from drive.MyDrive.RNN_sentiment_dataset.random_tweets import *\n",
        "\n",
        "# TF IDF Imports\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "from sklearn.model_selection import train_test_split  \n",
        "from scipy.sparse import csc_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from joblib import dump, load\n",
        "\n",
        "# RNN & LSTM Imports\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random_tweets imported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMN7fVescdmV"
      },
      "source": [
        "# Import du dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6BecGufb26s"
      },
      "source": [
        "DF = pd.read_csv('/content/drive/MyDrive/RNN_sentiment_dataset/tweets.csv',encoding='latin',usecols=[0, 5], # to take only 2 useful columns\n",
        "                 names=[\"label\",\"tweet\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPEG3XKecn7G"
      },
      "source": [
        "Dans ces données, nous avons y = 0 pour un sentiment négatif et y = 4 pour un sentiment positif. \n",
        "\n",
        "Nous ne garderons que le sentiment positif et négatif et transformerons y pour obtenir 0 pour un sentiment négatif et 1 pour un sentiment positif."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeEP8DU6dNPu"
      },
      "source": [
        "DF['label'].replace([4, 0],[1, 0], inplace=True) # Replace 0 and 4 by 0 and 1 to clarify"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2skzLFQPdW48"
      },
      "source": [
        "# Prétraitements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcy7VAOtdRfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33bf07a3-133c-40d6-fdce-4126198e6e30"
      },
      "source": [
        "  print(DF.head(25))\n",
        "  len(DF)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    label                                              tweet\n",
            "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
            "1       0  is upset that he can't update his Facebook by ...\n",
            "2       0  @Kenichan I dived many times for the ball. Man...\n",
            "3       0    my whole body feels itchy and like its on fire \n",
            "4       0  @nationwideclass no, it's not behaving at all....\n",
            "5       0                      @Kwesidei not the whole crew \n",
            "6       0                                        Need a hug \n",
            "7       0  @LOLTrish hey  long time no see! Yes.. Rains a...\n",
            "8       0               @Tatiana_K nope they didn't have it \n",
            "9       0                          @twittera que me muera ? \n",
            "10      0        spring break in plain city... it's snowing \n",
            "11      0                         I just re-pierced my ears \n",
            "12      0  @caregiving I couldn't bear to watch it.  And ...\n",
            "13      0  @octolinz16 It it counts, idk why I did either...\n",
            "14      0  @smarrison i would've been the first, but i di...\n",
            "15      0  @iamjazzyfizzle I wish I got to watch it with ...\n",
            "16      0  Hollis' death scene will hurt me severely to w...\n",
            "17      0                               about to file taxes \n",
            "18      0  @LettyA ahh ive always wanted to see rent  lov...\n",
            "19      0  @FakerPattyPattz Oh dear. Were you drinking ou...\n",
            "20      0  @alydesigns i was out most of the day so didn'...\n",
            "21      0  one of my friend called me, and asked to meet ...\n",
            "22      0   @angry_barista I baked you a cake but I ated it \n",
            "23      0             this week is not going as i had hoped \n",
            "24      0                         blagh class at 8 tomorrow \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1600000"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pakc8WAhdUCG"
      },
      "source": [
        "Nous pouvons voir que les tweets contiennent des mentions, des urls, etc. qui ne sont pas utiles pour le modèle de langage. \n",
        "\n",
        "Nous devons donc nettoyer leur contenu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D98xTSRbdxU5"
      },
      "source": [
        "## Nettoyer les tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PezobrTrda0I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1520901-bb26-4b5c-997c-44745c68d516"
      },
      "source": [
        "tokenizer = TweetTokenizer(strip_handles=True)\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "corpus = []"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBHJagcUd8JQ"
      },
      "source": [
        "Je crée une fonction pour nettoyer les tweets. Les contractions sont séparées, les caractères spéciaux sont supprimés, ainsi que les URL, les mentions, les mots trop courts et les mots vides."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN0WT1PUd4cm"
      },
      "source": [
        "def clean(tweet): \n",
        "            \n",
        "    # Contractions\n",
        "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
        "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
        "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
        "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
        "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
        "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
        "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
        "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
        "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
        "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
        "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
        "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
        "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
        "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
        "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
        "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
        "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
        "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
        "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
        "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
        "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
        "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
        "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
        "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
        "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
        "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
        "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
        "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
        "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
        "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
        "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
        "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
        "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
        "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
        "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
        "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
        "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
        "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
        "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
        "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
        "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
        "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
        "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
        "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
        "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
        "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
        "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
        "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
        "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
        "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
        "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
        "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
        "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
        "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
        "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
        "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
        "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
        "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
        "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
        "    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
        "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)  \n",
        "    \n",
        "    tweet = re.sub(r\"some1\", \"someone\", tweet)\n",
        "    tweet = re.sub(r\"yrs\", \"years\", tweet)\n",
        "    tweet = re.sub(r\"hrs\", \"hours\", tweet)\n",
        "    tweet = re.sub(r\"2morow|2moro\", \"tomorrow\", tweet)\n",
        "    tweet = re.sub(r\"2day\", \"today\", tweet)\n",
        "    tweet = re.sub(r\"4got|4gotten\", \"forget\", tweet)\n",
        "    tweet = re.sub(r\"b-day|bday\", \"b-day\", tweet)\n",
        "    tweet = re.sub(r\"mother's\", \"mother\", tweet)\n",
        "    tweet = re.sub(r\"mom's\", \"mom\", tweet)\n",
        "    tweet = re.sub(r\"dad's\", \"dad\", tweet)\n",
        "    tweet = re.sub(r\"hahah|hahaha|hahahaha\", \"haha\", tweet)\n",
        "    tweet = re.sub(r\"lmao|lolz|rofl\", \"lol\", tweet)\n",
        "    tweet = re.sub(r\"thanx|thnx\", \"thanks\", tweet)\n",
        "    tweet = re.sub(r\"goood\", \"good\", tweet)\n",
        "    tweet = re.sub(r\"some1\", \"someone\", tweet)\n",
        "    tweet = re.sub(r\"some1\", \"someone\", tweet)\n",
        "    # Character entity references\n",
        "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
        "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
        "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
        "    # Typos, slang and informal abbreviations\n",
        "    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
        "    tweet = re.sub(r\"w/\", \"with\", tweet)\n",
        "    tweet = re.sub(r\"<3\", \"love\", tweet)\n",
        "    # Urls\n",
        "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
        "    # Numbers\n",
        "    tweet = re.sub(r'[0-9]', '', tweet)\n",
        "    # Eliminating the mentions\n",
        "    tweet = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", tweet)\n",
        "    # Remove punctuation and special chars (keep '!')\n",
        "    for p in string.punctuation.replace('!', ''):\n",
        "        tweet = tweet.replace(p, '')\n",
        "        \n",
        "    # ... and ..\n",
        "    tweet = tweet.replace('...', ' ... ')\n",
        "    if '...' not in tweet:\n",
        "        tweet = tweet.replace('..', ' ... ')\n",
        "        \n",
        "    # Tokenize\n",
        "    tweet_words = tokenizer.tokenize(tweet)\n",
        "    \n",
        "    # Eliminating the word if its length is less than 3\n",
        "    tweet = [w for w in tweet_words if len(w)>2]\n",
        "    \n",
        "    # remove stopwords\n",
        "    tweet = [w.lower() for w in tweet if not w in stop_words]  \n",
        "    \n",
        "    corpus.append(tweet)\n",
        "    \n",
        "    # join back\n",
        "    tweet = ' '.join(tweet)\n",
        "        \n",
        "        \n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93unwggneKLH"
      },
      "source": [
        "Les abréviations seront remplacées par leur équivalent complet grâce à ce dictionnaire d'abréviations et à la fonction `convert_abbrev_in_text` associée"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95WVVnchd6KK"
      },
      "source": [
        "variable_name = \"\"\n",
        "abbreviations = {\n",
        "    \"$\" : \" dollar \",\n",
        "    \"€\" : \" euro \",\n",
        "    \"4ao\" : \"for adults only\",\n",
        "    \"a.m\" : \"before midday\",\n",
        "    \"a3\" : \"anytime anywhere anyplace\",\n",
        "    \"aamof\" : \"as a matter of fact\",\n",
        "    \"acct\" : \"account\",\n",
        "    \"adih\" : \"another day in hell\",\n",
        "    \"afaic\" : \"as far as i am concerned\",\n",
        "    \"afaict\" : \"as far as i can tell\",\n",
        "    \"afaik\" : \"as far as i know\",\n",
        "    \"afair\" : \"as far as i remember\",\n",
        "    \"afk\" : \"away from keyboard\",\n",
        "    \"app\" : \"application\",\n",
        "    \"approx\" : \"approximately\",\n",
        "    \"apps\" : \"applications\",\n",
        "    \"asap\" : \"as soon as possible\",\n",
        "    \"asl\" : \"age, sex, location\",\n",
        "    \"atk\" : \"at the keyboard\",\n",
        "    \"ave.\" : \"avenue\",\n",
        "    \"aymm\" : \"are you my mother\",\n",
        "    \"ayor\" : \"at your own risk\", \n",
        "    \"b&b\" : \"bed and breakfast\",\n",
        "    \"b+b\" : \"bed and breakfast\",\n",
        "    \"b.c\" : \"before christ\",\n",
        "    \"b2b\" : \"business to business\",\n",
        "    \"b2c\" : \"business to customer\",\n",
        "    \"b4\" : \"before\",\n",
        "    \"b4n\" : \"bye for now\",\n",
        "    \"b@u\" : \"back at you\",\n",
        "    \"bae\" : \"before anyone else\",\n",
        "    \"bak\" : \"back at keyboard\",\n",
        "    \"bbbg\" : \"bye bye be good\",\n",
        "    \"bbc\" : \"british broadcasting corporation\",\n",
        "    \"bbias\" : \"be back in a second\",\n",
        "    \"bbl\" : \"be back later\",\n",
        "    \"bbs\" : \"be back soon\",\n",
        "    \"be4\" : \"before\",\n",
        "    \"bfn\" : \"bye for now\",\n",
        "    \"blvd\" : \"boulevard\",\n",
        "    \"bout\" : \"about\",\n",
        "    \"brb\" : \"be right back\",\n",
        "    \"bros\" : \"brothers\",\n",
        "    \"brt\" : \"be right there\",\n",
        "    \"bsaaw\" : \"big smile and a wink\",\n",
        "    \"btw\" : \"by the way\",\n",
        "    \"bwl\" : \"bursting with laughter\",\n",
        "    \"c/o\" : \"care of\",\n",
        "    \"cet\" : \"central european time\",\n",
        "    \"cf\" : \"compare\",\n",
        "    \"cia\" : \"central intelligence agency\",\n",
        "    \"csl\" : \"can not stop laughing\",\n",
        "    \"cu\" : \"see you\",\n",
        "    \"cul8r\" : \"see you later\",\n",
        "    \"cv\" : \"curriculum vitae\",\n",
        "    \"cwot\" : \"complete waste of time\",\n",
        "    \"cya\" : \"see you\",\n",
        "    \"cyt\" : \"see you tomorrow\",\n",
        "    \"dae\" : \"does anyone else\",\n",
        "    \"dbmib\" : \"do not bother me i am busy\",\n",
        "    \"diy\" : \"do it yourself\",\n",
        "    \"dm\" : \"direct message\",\n",
        "    \"dwh\" : \"during work hours\",\n",
        "    \"e123\" : \"easy as one two three\",\n",
        "    \"eet\" : \"eastern european time\",\n",
        "    \"eg\" : \"example\",\n",
        "    \"embm\" : \"early morning business meeting\",\n",
        "    \"encl\" : \"enclosed\",\n",
        "    \"encl.\" : \"enclosed\",\n",
        "    \"etc\" : \"and so on\",\n",
        "    \"faq\" : \"frequently asked questions\",\n",
        "    \"fawc\" : \"for anyone who cares\",\n",
        "    \"fb\" : \"facebook\",\n",
        "    \"fc\" : \"fingers crossed\",\n",
        "    \"fig\" : \"figure\",\n",
        "    \"fimh\" : \"forever in my heart\", \n",
        "    \"ft.\" : \"feet\",\n",
        "    \"ft\" : \"featuring\",\n",
        "    \"ftl\" : \"for the loss\",\n",
        "    \"ftw\" : \"for the win\",\n",
        "    \"fwiw\" : \"for what it is worth\",\n",
        "    \"fyi\" : \"for your information\",\n",
        "    \"g9\" : \"genius\",\n",
        "    \"gahoy\" : \"get a hold of yourself\",\n",
        "    \"gal\" : \"get a life\",\n",
        "    \"gcse\" : \"general certificate of secondary education\",\n",
        "    \"gfn\" : \"gone for now\",\n",
        "    \"gg\" : \"good game\",\n",
        "    \"gl\" : \"good luck\",\n",
        "    \"glhf\" : \"good luck have fun\",\n",
        "    \"gmt\" : \"greenwich mean time\",\n",
        "    \"gmta\" : \"great minds think alike\",\n",
        "    \"gn\" : \"good night\",\n",
        "    \"g.o.a.t\" : \"greatest of all time\",\n",
        "    \"goat\" : \"greatest of all time\",\n",
        "    \"goi\" : \"get over it\",\n",
        "    \"gps\" : \"global positioning system\",\n",
        "    \"gr8\" : \"great\",\n",
        "    \"gratz\" : \"congratulations\",\n",
        "    \"gyal\" : \"girl\",\n",
        "    \"h&c\" : \"hot and cold\",\n",
        "    \"hp\" : \"horsepower\",\n",
        "    \"hr\" : \"hour\",\n",
        "    \"hrh\" : \"his royal highness\",\n",
        "    \"ht\" : \"height\",\n",
        "    \"ibrb\" : \"i will be right back\",\n",
        "    \"ic\" : \"i see\",\n",
        "    \"icq\" : \"i seek you\",\n",
        "    \"icymi\" : \"in case you missed it\",\n",
        "    \"idc\" : \"i do not care\",\n",
        "    \"idgadf\" : \"i do not give a damn fuck\",\n",
        "    \"idgaf\" : \"i do not give a fuck\",\n",
        "    \"idk\" : \"i do not know\",\n",
        "    \"ie\" : \"that is\",\n",
        "    \"i.e\" : \"that is\",\n",
        "    \"ifyp\" : \"i feel your pain\",\n",
        "    \"IG\" : \"instagram\",\n",
        "    \"iirc\" : \"if i remember correctly\",\n",
        "    \"ilu\" : \"i love you\",\n",
        "    \"ily\" : \"i love you\",\n",
        "    \"imho\" : \"in my humble opinion\",\n",
        "    \"imo\" : \"in my opinion\",\n",
        "    \"imu\" : \"i miss you\",\n",
        "    \"iow\" : \"in other words\",\n",
        "    \"irl\" : \"in real life\",\n",
        "    \"j4f\" : \"just for fun\",\n",
        "    \"jic\" : \"just in case\",\n",
        "    \"jk\" : \"just kidding\",\n",
        "    \"jsyk\" : \"just so you know\",\n",
        "    \"l8r\" : \"later\",\n",
        "    \"lb\" : \"pound\",\n",
        "    \"lbs\" : \"pounds\",\n",
        "    \"ldr\" : \"long distance relationship\",\n",
        "    \"lmao\" : \"laugh my ass off\",\n",
        "    \"lmfao\" : \"laugh my fucking ass off\",\n",
        "    \"lol\" : \"laughing out loud\",\n",
        "    \"ltd\" : \"limited\",\n",
        "    \"ltns\" : \"long time no see\",\n",
        "    \"m8\" : \"mate\",\n",
        "    \"mf\" : \"motherfucker\",\n",
        "    \"mfs\" : \"motherfuckers\",\n",
        "    \"mfw\" : \"my face when\",\n",
        "    \"mofo\" : \"motherfucker\",\n",
        "    \"mph\" : \"miles per hour\",\n",
        "    \"mr\" : \"mister\",\n",
        "    \"mrw\" : \"my reaction when\",\n",
        "    \"ms\" : \"miss\",\n",
        "    \"mte\" : \"my thoughts exactly\",\n",
        "    \"nagi\" : \"not a good idea\",\n",
        "    \"nbc\" : \"national broadcasting company\",\n",
        "    \"nbd\" : \"not big deal\",\n",
        "    \"nfs\" : \"not for sale\",\n",
        "    \"ngl\" : \"not going to lie\",\n",
        "    \"nhs\" : \"national health service\",\n",
        "    \"nrn\" : \"no reply necessary\",\n",
        "    \"nsfl\" : \"not safe for life\",\n",
        "    \"nsfw\" : \"not safe for work\",\n",
        "    \"nth\" : \"nice to have\",\n",
        "    \"nvr\" : \"never\",\n",
        "    \"nyc\" : \"new york city\",\n",
        "    \"oc\" : \"original content\",\n",
        "    \"og\" : \"original\",\n",
        "    \"ohp\" : \"overhead projector\",\n",
        "    \"oic\" : \"oh i see\",\n",
        "    \"omdb\" : \"over my dead body\",\n",
        "    \"omg\" : \"oh my god\",\n",
        "    \"omw\" : \"on my way\",\n",
        "    \"p.a\" : \"per annum\",\n",
        "    \"p.m\" : \"after midday\",\n",
        "    \"pm\" : \"prime minister\",\n",
        "    \"poc\" : \"people of color\",\n",
        "    \"pov\" : \"point of view\",\n",
        "    \"pp\" : \"pages\",\n",
        "    \"ppl\" : \"people\",\n",
        "    \"prw\" : \"parents are watching\",\n",
        "    \"ps\" : \"postscript\",\n",
        "    \"pt\" : \"point\",\n",
        "    \"ptb\" : \"please text back\",\n",
        "    \"pto\" : \"please turn over\",\n",
        "    \"qpsa\" : \"what happens\", \n",
        "    \"ratchet\" : \"rude\",\n",
        "    \"rbtl\" : \"read between the lines\",\n",
        "    \"rlrt\" : \"real life retweet\", \n",
        "    \"rofl\" : \"rolling on the floor laughing\",\n",
        "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
        "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
        "    \"rt\" : \"retweet\",\n",
        "    \"ruok\" : \"are you ok\",\n",
        "    \"sfw\" : \"safe for work\",\n",
        "     \"sk8\" : \"skate\",\n",
        "    \"smh\" : \"shake my head\",\n",
        "    \"sq\" : \"square\",\n",
        "    \"srsly\" : \"seriously\", \n",
        "    \"ssdd\" : \"same stuff different day\",\n",
        "    \"tbh\" : \"to be honest\",\n",
        "    \"tbs\" : \"tablespooful\",\n",
        "    \"tbsp\" : \"tablespooful\",\n",
        "    \"tfw\" : \"that feeling when\",\n",
        "    \"thks\" : \"thank you\",\n",
        "    \"tho\" : \"though\",\n",
        "    \"thx\" : \"thank you\",\n",
        "    \"tia\" : \"thanks in advance\",\n",
        "    \"til\" : \"today i learned\",\n",
        "    \"tl;dr\" : \"too long i did not read\",\n",
        "    \"tldr\" : \"too long i did not read\",\n",
        "    \"tmb\" : \"tweet me back\",\n",
        "    \"tntl\" : \"trying not to laugh\",\n",
        "    \"ttyl\" : \"talk to you later\",\n",
        "    \"u\" : \"you\",\n",
        "    \"u2\" : \"you too\",\n",
        "    \"u4e\" : \"yours for ever\",\n",
        "    \"utc\" : \"coordinated universal time\",\n",
        "    \"w/\" : \"with\",\n",
        "    \"w/o\" : \"without\",\n",
        "    \"w8\" : \"wait\",\n",
        "    \"wassup\" : \"what is up\",\n",
        "    \"wb\" : \"welcome back\",\n",
        "    \"wtf\" : \"what the fuck\",\n",
        "    \"wtg\" : \"way to go\",\n",
        "    \"wtpa\" : \"where the party at\",\n",
        "    \"wuf\" : \"where are you from\",\n",
        "    \"wuzup\" : \"what is up\",\n",
        "    \"wywh\" : \"wish you were here\",\n",
        "    \"yd\" : \"yard\",\n",
        "    \"ygtr\" : \"you got that right\",\n",
        "    \"ynk\" : \"you never know\",\n",
        "    \"zzz\" : \"sleeping bored and tired\"\n",
        "}\n",
        "\n",
        "def convert_abbrev_in_text(tweet):\n",
        "    t=[]\n",
        "    words=tweet.split()\n",
        "    t = [abbreviations[w.lower()] if w.lower() in abbreviations.keys() else w for w in words]\n",
        "    return ' '.join(t) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wy3Wq-aeONJ"
      },
      "source": [
        "La fonction suivante exécute les deux fonctions définies ci-dessus sur un tweet donné :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otc2qVcbeL6B"
      },
      "source": [
        "def prepare_string(tweet):\n",
        "  tweet = clean(tweet)\n",
        "  tweet = convert_abbrev_in_text(tweet)\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al9EVADbeRsY"
      },
      "source": [
        "Cette étape peut prendre quelques minutes, elle applique la fonction de nettoyage à tous les tweets du corpus de texte et supprime les lignes qui sont vides après le nettoyage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Iq7Gr0ZeQL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edea176b-0a20-467b-b944-e2564fc806e7"
      },
      "source": [
        "%%time\n",
        "# Apply prepare_string to all rows in 'tweets' column\n",
        "DF['tweet'] = DF['tweet'].apply(lambda s : prepare_string(s))\n",
        "\n",
        "# Drop empty values from dataframe\n",
        "DF['tweet'].replace('', np.nan, inplace=True)\n",
        "DF.dropna(subset=['tweet'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5min 47s, sys: 2.38 s, total: 5min 50s\n",
            "Wall time: 5min 56s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kon3bKO3eXIc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "outputId": "8086f516-947a-43ec-e1a0-65a2c7f353e8"
      },
      "source": [
        "DF.head(25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-58c24a9c-d8e9-4c5d-8cb9-76d4b9c16b31\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>awww bummer you shoulda got david carr third day</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>upset cannot update facebook texting might cry...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>dived many times ball managed save the rest bo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>whole body feels itchy like fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>behaving mad cannot see</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>whole crew</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>need hug</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>hey long time see yes rains bit bit laughing o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>nope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>que muera</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>spring break plain city snowing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>repierced ears</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>could bear watch and thought loss embarrassing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>counts i do not know either never talk anymore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>would first gun really though zac snyders douc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>wish got watch miss premiere</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>hollis death scene hurt severely watch film wr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>file taxes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>ahh ive always wanted see rent love soundtrack</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>dear were drinking forgotten table drinks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0</td>\n",
              "      <td>day get much done</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0</td>\n",
              "      <td>one friend called asked meet mid valley todayb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0</td>\n",
              "      <td>baked cake ated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0</td>\n",
              "      <td>week going hoped</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0</td>\n",
              "      <td>blagh class tomorrow</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58c24a9c-d8e9-4c5d-8cb9-76d4b9c16b31')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-58c24a9c-d8e9-4c5d-8cb9-76d4b9c16b31 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-58c24a9c-d8e9-4c5d-8cb9-76d4b9c16b31');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    label                                              tweet\n",
              "0       0   awww bummer you shoulda got david carr third day\n",
              "1       0  upset cannot update facebook texting might cry...\n",
              "2       0  dived many times ball managed save the rest bo...\n",
              "3       0                   whole body feels itchy like fire\n",
              "4       0                            behaving mad cannot see\n",
              "5       0                                         whole crew\n",
              "6       0                                           need hug\n",
              "7       0  hey long time see yes rains bit bit laughing o...\n",
              "8       0                                               nope\n",
              "9       0                                          que muera\n",
              "10      0                    spring break plain city snowing\n",
              "11      0                                     repierced ears\n",
              "12      0     could bear watch and thought loss embarrassing\n",
              "13      0     counts i do not know either never talk anymore\n",
              "14      0  would first gun really though zac snyders douc...\n",
              "15      0                       wish got watch miss premiere\n",
              "16      0  hollis death scene hurt severely watch film wr...\n",
              "17      0                                         file taxes\n",
              "18      0     ahh ive always wanted see rent love soundtrack\n",
              "19      0          dear were drinking forgotten table drinks\n",
              "20      0                                  day get much done\n",
              "21      0  one friend called asked meet mid valley todayb...\n",
              "22      0                                    baked cake ated\n",
              "23      0                                   week going hoped\n",
              "24      0                               blagh class tomorrow"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RQiDkWXeb7d"
      },
      "source": [
        "Le DataFrame résultant est converti en CSV et téléchargé afin d'éviter la réexécution du code précédent qui consomme beaucoup de ressources et de temps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyyYaavmecNj"
      },
      "source": [
        "DF.to_csv('/content/drive/MyDrive/Colab Notebooks/cleaned_tweets.csv', index=False)\n",
        "DF = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/cleaned_tweets.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFj93f1Eeu7x"
      },
      "source": [
        "Les données sont maintenant prêtes à être soumises aux différentes méthodes de traitement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b072kCce8Ip"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUCSoW0Kfg9T"
      },
      "source": [
        "# Using all the data exceeds the RAM capacity of the Notebook \n",
        "corpus_size = int(20000)\n",
        "\n",
        "# Tweets are chosen at the beginning and end of the dataset to have equal parts positive and negative sentiment\n",
        "tweets = [*DF['tweet'].values[:int(corpus_size/2)], *DF['tweet'].values[-int(corpus_size/2):]]\n",
        "# As well as for the associated targets\n",
        "y = [*DF['label'].values[:int(corpus_size/2)], *DF['label'].values[-int(corpus_size/2):]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPpVRwFzfHj7"
      },
      "source": [
        "Utilisez la fonction TF-IDF de Sklearn.\n",
        "\n",
        "Aidez-vous de la [doc](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyPyP9t7fTv-"
      },
      "source": [
        "tfIdfVectorizer = TfidfVectorizer()\n",
        "X = tfIdfVectorizer.fit_transform(tweets).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM_CSNzefvRl"
      },
      "source": [
        "Divisez votre ensemble de données de formation et de test à l'aide de la fonction sklearn. \n",
        "\n",
        "Aidez-vous de la [doc](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFKso6IbexJP"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.025, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouwTGyudgAOM"
      },
      "source": [
        "Entraînez un modèle en utilisant la fonction de forêt aléatoire de sklearn.\n",
        "\n",
        "Aidez-vous de la [doc](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1Ft6Ixrf-DU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f881a4da-0de3-45c2-9a33-42068e8c4f0d"
      },
      "source": [
        "# Use only 15 estimator to save your ram\n",
        "text_classifier = RandomForestClassifier(n_estimators=15, random_state=0)  \n",
        "text_classifier.fit(X_train, y_train )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(n_estimators=15, random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m6k9TJzg6eL"
      },
      "source": [
        "Calculer la prédiction sur l'ensemble de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5BWYCO_g9Zq"
      },
      "source": [
        "predictions = text_classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVyMm31-gUp8"
      },
      "source": [
        "Calculer quelques mesures de performance : \n",
        "- matrice de confusion ([doc](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)) ;\n",
        "- rapport de classification ([doc](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)) ;\n",
        "- précision ([doc](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5hO6AHdgTRV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae765739-d5da-48fe-934d-dff50a2ed774"
      },
      "source": [
        "print(confusion_matrix(y_test,predictions)) \n",
        "\n",
        "print(classification_report(y_test,predictions))  \n",
        "\n",
        "print(accuracy_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[159  82]\n",
            " [ 59 200]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.66      0.69       241\n",
            "           1       0.71      0.77      0.74       259\n",
            "\n",
            "    accuracy                           0.72       500\n",
            "   macro avg       0.72      0.72      0.72       500\n",
            "weighted avg       0.72      0.72      0.72       500\n",
            "\n",
            "0.718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGdxbSeZhNNs"
      },
      "source": [
        "# RNN & LSTM with GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAWyNW4Rg_gu"
      },
      "source": [
        "# Creation of tuples (tweet, label) in anticipation of data mixing\n",
        "corpus = list(zip(DF['tweet'].values, DF['label'].values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWZKNQ9rhvqP"
      },
      "source": [
        "Hyperparamètres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfbUASlrhs6n"
      },
      "source": [
        "embedding_dim = 100    # Dimensions used for glove6b100\n",
        "max_length = 20        # Maximum size of a tweet\n",
        "trunc_type='post'      # Truncates the tweet if it is longer than max_length\n",
        "padding_type='post'    # Adds padding to the end of the tweet if it is shorter than max_length\n",
        "oov_tok = \"<OOV>\"      # Token \"<OOV>\" replaces words that are not part of the vocabulary (Out Of Vocabulary)\n",
        "training_size=len(corpus)\n",
        "test_portion=.025"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kzORRyLh9Xz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6e9386a-0346-4227-bfad-c3d41863dd8f"
      },
      "source": [
        "corpus[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('awww bummer you shoulda got david carr third day', 0),\n",
              " ('upset cannot update facebook texting might cry result school today also blah',\n",
              "  0),\n",
              " ('dived many times ball managed save the rest bounds', 0),\n",
              " ('whole body feels itchy like fire', 0),\n",
              " ('behaving mad cannot see', 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9DAK9efiTO1"
      },
      "source": [
        "#### Distribution des données de test et d'entraînement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVDjbHDKiM8B"
      },
      "source": [
        "sentences=[]\n",
        "labels=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9mVoqfAiVp3"
      },
      "source": [
        "# Mélange des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY6gkarsiUzZ"
      },
      "source": [
        "# Your code\n",
        "random.shuffle(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFkoKvDPidwr"
      },
      "source": [
        "Séparation des labels et des tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bf4GCv3icJu"
      },
      "source": [
        "# Your code\n",
        "for line in corpus:\n",
        "  sentences.append(line[0])\n",
        "  labels.append(line[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNvJKFNSij-S"
      },
      "source": [
        "Tonkenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd5-3go2iiiz"
      },
      "source": [
        "# Your code\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yARuEZn8iupF"
      },
      "source": [
        "Création du vecteur de mapping avec les mots de vocabulaire associés aux indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piqgvzJRiozW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08e06888-21ae-4a9d-c098-394cd4c2639a"
      },
      "source": [
        "# Your code\n",
        "tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'good': 1,\n",
              " 'day': 2,\n",
              " 'get': 3,\n",
              " 'like': 4,\n",
              " 'love': 5,\n",
              " 'today': 6,\n",
              " 'going': 7,\n",
              " 'work': 8,\n",
              " 'out': 9,\n",
              " 'got': 10,\n",
              " 'laughing': 11,\n",
              " 'loud': 12,\n",
              " 'back': 13,\n",
              " 'time': 14,\n",
              " 'know': 15,\n",
              " 'one': 16,\n",
              " 'really': 17,\n",
              " 'cannot': 18,\n",
              " 'see': 19,\n",
              " 'new': 20,\n",
              " 'still': 21,\n",
              " 'want': 22,\n",
              " 'well': 23,\n",
              " 'night': 24,\n",
              " 'think': 25,\n",
              " 'thanks': 26,\n",
              " 'the': 27,\n",
              " 'would': 28,\n",
              " 'home': 29,\n",
              " 'much': 30,\n",
              " 'miss': 31,\n",
              " 'need': 32,\n",
              " 'you': 33,\n",
              " 'last': 34,\n",
              " 'tomorrow': 35,\n",
              " 'morning': 36,\n",
              " 'hope': 37,\n",
              " 'great': 38,\n",
              " 'haha': 39,\n",
              " 'twitter': 40,\n",
              " 'though': 41,\n",
              " 'feel': 42,\n",
              " 'way': 43,\n",
              " 'sad': 44,\n",
              " 'just': 45,\n",
              " 'fun': 46,\n",
              " 'wish': 47,\n",
              " 'right': 48,\n",
              " 'sleep': 49,\n",
              " 'bad': 50,\n",
              " 'happy': 51,\n",
              " 'could': 52,\n",
              " 'sorry': 53,\n",
              " 'tonight': 54,\n",
              " 'come': 55,\n",
              " 'make': 56,\n",
              " 'getting': 57,\n",
              " 'gonna': 58,\n",
              " 'people': 59,\n",
              " 'nice': 60,\n",
              " 'better': 61,\n",
              " 'and': 62,\n",
              " 'watching': 63,\n",
              " 'wait': 64,\n",
              " 'yeah': 65,\n",
              " 'bed': 66,\n",
              " 'week': 67,\n",
              " 'god': 68,\n",
              " 'thank': 69,\n",
              " 'school': 70,\n",
              " 'hate': 71,\n",
              " 'but': 72,\n",
              " 'days': 73,\n",
              " 'even': 74,\n",
              " 'hey': 75,\n",
              " 'next': 76,\n",
              " 'soon': 77,\n",
              " 'dont': 78,\n",
              " 'yes': 79,\n",
              " 'weekend': 80,\n",
              " 'cant': 81,\n",
              " 'awesome': 82,\n",
              " 'never': 83,\n",
              " 'not': 84,\n",
              " 'that': 85,\n",
              " 'take': 86,\n",
              " 'little': 87,\n",
              " 'long': 88,\n",
              " 'first': 89,\n",
              " 'working': 90,\n",
              " 'wanna': 91,\n",
              " 'say': 92,\n",
              " 'best': 93,\n",
              " 'please': 94,\n",
              " 'show': 95,\n",
              " 'what': 96,\n",
              " 'tired': 97,\n",
              " 'sick': 98,\n",
              " 'life': 99,\n",
              " 'watch': 100,\n",
              " 'hours': 101,\n",
              " 'everyone': 102,\n",
              " 'my': 103,\n",
              " 'done': 104,\n",
              " 'feeling': 105,\n",
              " 'always': 106,\n",
              " 'sure': 107,\n",
              " 'friends': 108,\n",
              " 'let': 109,\n",
              " 'already': 110,\n",
              " 'thing': 111,\n",
              " 'another': 112,\n",
              " 'find': 113,\n",
              " 'cool': 114,\n",
              " 'something': 115,\n",
              " 'guys': 116,\n",
              " 'ready': 117,\n",
              " 'made': 118,\n",
              " 'looking': 119,\n",
              " 'yay': 120,\n",
              " 'went': 121,\n",
              " 'man': 122,\n",
              " 'look': 123,\n",
              " 'phone': 124,\n",
              " 'yet': 125,\n",
              " 'house': 126,\n",
              " 'movie': 127,\n",
              " 'ever': 128,\n",
              " 'pretty': 129,\n",
              " 'trying': 130,\n",
              " 'have': 131,\n",
              " 'away': 132,\n",
              " 'maybe': 133,\n",
              " 'oh': 134,\n",
              " 'finally': 135,\n",
              " 'old': 136,\n",
              " 'help': 137,\n",
              " 'summer': 138,\n",
              " 'amazing': 139,\n",
              " 'early': 140,\n",
              " 'things': 141,\n",
              " 'left': 142,\n",
              " 'lost': 143,\n",
              " 'tweet': 144,\n",
              " 'someone': 145,\n",
              " 'now': 146,\n",
              " 'follow': 147,\n",
              " 'guess': 148,\n",
              " 'damn': 149,\n",
              " 'keep': 150,\n",
              " 'thought': 151,\n",
              " 'big': 152,\n",
              " 'bit': 153,\n",
              " 'missed': 154,\n",
              " 'hot': 155,\n",
              " 'sucks': 156,\n",
              " 'year': 157,\n",
              " 'this': 158,\n",
              " 'nothing': 159,\n",
              " 'rain': 160,\n",
              " 'start': 161,\n",
              " 'friend': 162,\n",
              " 'wow': 163,\n",
              " 'glad': 164,\n",
              " 'try': 165,\n",
              " 'later': 166,\n",
              " 'bored': 167,\n",
              " 'coming': 168,\n",
              " 'also': 169,\n",
              " 'tell': 170,\n",
              " 'looks': 171,\n",
              " 'birthday': 172,\n",
              " 'live': 173,\n",
              " 'two': 174,\n",
              " 'hear': 175,\n",
              " 'girl': 176,\n",
              " 'weather': 177,\n",
              " 'actually': 178,\n",
              " 'baby': 179,\n",
              " 'saw': 180,\n",
              " 'song': 181,\n",
              " 'sun': 182,\n",
              " 'makes': 183,\n",
              " 'stuff': 184,\n",
              " 'excited': 185,\n",
              " 'might': 186,\n",
              " 'waiting': 187,\n",
              " 'party': 188,\n",
              " 'said': 189,\n",
              " 'hard': 190,\n",
              " 'play': 191,\n",
              " 'thats': 192,\n",
              " 'game': 193,\n",
              " 'since': 194,\n",
              " 'how': 195,\n",
              " 'ugh': 196,\n",
              " 'head': 197,\n",
              " 'lot': 198,\n",
              " 'yesterday': 199,\n",
              " 'late': 200,\n",
              " 'gotta': 201,\n",
              " 'around': 202,\n",
              " 'world': 203,\n",
              " 'many': 204,\n",
              " 'car': 205,\n",
              " 'sounds': 206,\n",
              " 'music': 207,\n",
              " 'found': 208,\n",
              " 'luck': 209,\n",
              " 'mom': 210,\n",
              " 'talk': 211,\n",
              " 'check': 212,\n",
              " 'give': 213,\n",
              " 'job': 214,\n",
              " 'beautiful': 215,\n",
              " 'why': 216,\n",
              " 'must': 217,\n",
              " 'friday': 218,\n",
              " 'read': 219,\n",
              " 'cold': 220,\n",
              " 'making': 221,\n",
              " 'call': 222,\n",
              " 'its': 223,\n",
              " 'put': 224,\n",
              " 'gone': 225,\n",
              " 'may': 226,\n",
              " 'sunday': 227,\n",
              " 'aww': 228,\n",
              " 'missing': 229,\n",
              " 'i': 230,\n",
              " 'anything': 231,\n",
              " 'least': 232,\n",
              " 'poor': 233,\n",
              " 'woke': 234,\n",
              " 'till': 235,\n",
              " 'stop': 236,\n",
              " 'monday': 237,\n",
              " 'leave': 238,\n",
              " 'use': 239,\n",
              " 'almost': 240,\n",
              " 'times': 241,\n",
              " 'hour': 242,\n",
              " 'okay': 243,\n",
              " 'listening': 244,\n",
              " 'cute': 245,\n",
              " 'far': 246,\n",
              " 'hair': 247,\n",
              " 'mean': 248,\n",
              " 'wanted': 249,\n",
              " 'hurts': 250,\n",
              " 'lunch': 251,\n",
              " 'iphone': 252,\n",
              " 'eat': 253,\n",
              " 'free': 254,\n",
              " 'family': 255,\n",
              " 'fuck': 256,\n",
              " 'funny': 257,\n",
              " 'enjoy': 258,\n",
              " 'without': 259,\n",
              " 'food': 260,\n",
              " 'finished': 261,\n",
              " 'end': 262,\n",
              " 'dinner': 263,\n",
              " 'believe': 264,\n",
              " 'anyone': 265,\n",
              " 'playing': 266,\n",
              " 'forward': 267,\n",
              " 'welcome': 268,\n",
              " 'followers': 269,\n",
              " 'shit': 270,\n",
              " 'thinking': 271,\n",
              " 'everything': 272,\n",
              " 'sweet': 273,\n",
              " 'win': 274,\n",
              " 'every': 275,\n",
              " 'off': 276,\n",
              " 'cause': 277,\n",
              " 'they': 278,\n",
              " 'totally': 279,\n",
              " 'video': 280,\n",
              " 'didnt': 281,\n",
              " 'buy': 282,\n",
              " 'real': 283,\n",
              " 'outside': 284,\n",
              " 'enough': 285,\n",
              " 'ill': 286,\n",
              " 'weeks': 287,\n",
              " 'stupid': 288,\n",
              " 'all': 289,\n",
              " 'mine': 290,\n",
              " 'coffee': 291,\n",
              " 'wrong': 292,\n",
              " 'ass': 293,\n",
              " 'years': 294,\n",
              " 'anymore': 295,\n",
              " 'probably': 296,\n",
              " 'place': 297,\n",
              " 'eating': 298,\n",
              " 'room': 299,\n",
              " 'wants': 300,\n",
              " 'stay': 301,\n",
              " 'tweets': 302,\n",
              " 'money': 303,\n",
              " 'sooo': 304,\n",
              " 'busy': 305,\n",
              " 'following': 306,\n",
              " 'lovely': 307,\n",
              " 'dad': 308,\n",
              " 'whole': 309,\n",
              " 'came': 310,\n",
              " 'seen': 311,\n",
              " 'about': 312,\n",
              " 'says': 313,\n",
              " 'taking': 314,\n",
              " 'saturday': 315,\n",
              " 'pic': 316,\n",
              " 'kids': 317,\n",
              " 'kinda': 318,\n",
              " 'class': 319,\n",
              " 'for': 320,\n",
              " 'exam': 321,\n",
              " 'beach': 322,\n",
              " 'had': 323,\n",
              " 'took': 324,\n",
              " 'hopefully': 325,\n",
              " 'headache': 326,\n",
              " 'crazy': 327,\n",
              " 'super': 328,\n",
              " 'name': 329,\n",
              " 'idea': 330,\n",
              " 'hello': 331,\n",
              " 'hell': 332,\n",
              " 'able': 333,\n",
              " 'news': 334,\n",
              " 'half': 335,\n",
              " 'true': 336,\n",
              " 'forgot': 337,\n",
              " 'guy': 338,\n",
              " 'book': 339,\n",
              " 'goodnight': 340,\n",
              " 'learned': 341,\n",
              " 'meet': 342,\n",
              " 'lots': 343,\n",
              " 'awww': 344,\n",
              " 'post': 345,\n",
              " 'face': 346,\n",
              " 'leaving': 347,\n",
              " 'wont': 348,\n",
              " 'girls': 349,\n",
              " 'sitting': 350,\n",
              " 'else': 351,\n",
              " 'city': 352,\n",
              " 'she': 353,\n",
              " 'hahaa': 354,\n",
              " 'send': 355,\n",
              " 'ago': 356,\n",
              " 'rest': 357,\n",
              " 'either': 358,\n",
              " 'used': 359,\n",
              " 'reading': 360,\n",
              " 'soo': 361,\n",
              " 'full': 362,\n",
              " 'feels': 363,\n",
              " 'shopping': 364,\n",
              " 'hurt': 365,\n",
              " 'run': 366,\n",
              " 'computer': 367,\n",
              " 'seems': 368,\n",
              " 'did': 369,\n",
              " 'talking': 370,\n",
              " 'cuz': 371,\n",
              " 'watched': 372,\n",
              " 'remember': 373,\n",
              " 'raining': 374,\n",
              " 'tried': 375,\n",
              " 'hit': 376,\n",
              " 'needs': 377,\n",
              " 'stuck': 378,\n",
              " 'alone': 379,\n",
              " 'heard': 380,\n",
              " 'blog': 381,\n",
              " 'trip': 382,\n",
              " 'office': 383,\n",
              " 'boo': 384,\n",
              " 'started': 385,\n",
              " 'by': 386,\n",
              " 'dog': 387,\n",
              " 'kind': 388,\n",
              " 'heart': 389,\n",
              " 'course': 390,\n",
              " 'will': 391,\n",
              " 'seeing': 392,\n",
              " 'hehe': 393,\n",
              " 'internet': 394,\n",
              " 'part': 395,\n",
              " 'mind': 396,\n",
              " 'there': 397,\n",
              " 'using': 398,\n",
              " 'seriously': 399,\n",
              " 'quite': 400,\n",
              " 'fucking': 401,\n",
              " 'online': 402,\n",
              " 'picture': 403,\n",
              " 'told': 404,\n",
              " 'add': 405,\n",
              " 'too': 406,\n",
              " 'awake': 407,\n",
              " 'pics': 408,\n",
              " 'loved': 409,\n",
              " 'quot': 410,\n",
              " 'boy': 411,\n",
              " 'your': 412,\n",
              " 'goes': 413,\n",
              " 'laugh': 414,\n",
              " 'cry': 415,\n",
              " 'fine': 416,\n",
              " 'pain': 417,\n",
              " 'break': 418,\n",
              " 'breakfast': 419,\n",
              " 'change': 420,\n",
              " 'gets': 421,\n",
              " 'wake': 422,\n",
              " 'person': 423,\n",
              " 'bought': 424,\n",
              " 'sunny': 425,\n",
              " 'care': 426,\n",
              " 'boring': 427,\n",
              " 'minutes': 428,\n",
              " 'broke': 429,\n",
              " 'update': 430,\n",
              " 'called': 431,\n",
              " 'facebook': 432,\n",
              " 'season': 433,\n",
              " 'starting': 434,\n",
              " 'concert': 435,\n",
              " 'pay': 436,\n",
              " 'open': 437,\n",
              " 'lucky': 438,\n",
              " 'june': 439,\n",
              " 'asleep': 440,\n",
              " 'dude': 441,\n",
              " 'sleeping': 442,\n",
              " 'bring': 443,\n",
              " 'favorite': 444,\n",
              " 'link': 445,\n",
              " 'nite': 446,\n",
              " 'anyway': 447,\n",
              " 'hungry': 448,\n",
              " 'site': 449,\n",
              " 'crap': 450,\n",
              " 'heading': 451,\n",
              " 'reply': 452,\n",
              " 'month': 453,\n",
              " 'instead': 454,\n",
              " 'email': 455,\n",
              " 'walk': 456,\n",
              " 'train': 457,\n",
              " 'study': 458,\n",
              " 'afternoon': 459,\n",
              " 'drive': 460,\n",
              " 'shower': 461,\n",
              " 'fan': 462,\n",
              " 'jealous': 463,\n",
              " 'enjoying': 464,\n",
              " 'exams': 465,\n",
              " 'red': 466,\n",
              " 'text': 467,\n",
              " 'wonderful': 468,\n",
              " 'mad': 469,\n",
              " 'definitely': 470,\n",
              " 'sore': 471,\n",
              " 'hoping': 472,\n",
              " 'yea': 473,\n",
              " 'ice': 474,\n",
              " 'move': 475,\n",
              " 'soooo': 476,\n",
              " 'bye': 477,\n",
              " 'together': 478,\n",
              " 'rock': 479,\n",
              " 'running': 480,\n",
              " 'finish': 481,\n",
              " 'bday': 482,\n",
              " 'problem': 483,\n",
              " 'congrats': 484,\n",
              " 'died': 485,\n",
              " 'high': 486,\n",
              " 'means': 487,\n",
              " 'ask': 488,\n",
              " 'works': 489,\n",
              " 'very': 490,\n",
              " 'ones': 491,\n",
              " 'dead': 492,\n",
              " 'happened': 493,\n",
              " 'goin': 494,\n",
              " 'fail': 495,\n",
              " 'sister': 496,\n",
              " 'was': 497,\n",
              " 'sometimes': 498,\n",
              " 'homework': 499,\n",
              " 'couple': 500,\n",
              " 'write': 501,\n",
              " 'boys': 502,\n",
              " 'dear': 503,\n",
              " 'movies': 504,\n",
              " 'album': 505,\n",
              " 'months': 506,\n",
              " 'drink': 507,\n",
              " 'suck': 508,\n",
              " 'comes': 509,\n",
              " 'loves': 510,\n",
              " 'top': 511,\n",
              " 'cut': 512,\n",
              " 'are': 513,\n",
              " 'laptop': 514,\n",
              " 'brother': 515,\n",
              " 'doesnt': 516,\n",
              " 'set': 517,\n",
              " 'ive': 518,\n",
              " 'eyes': 519,\n",
              " 'youtube': 520,\n",
              " 'church': 521,\n",
              " 'tour': 522,\n",
              " 'reason': 523,\n",
              " 'ipod': 524,\n",
              " 'sound': 525,\n",
              " 'happen': 526,\n",
              " 'water': 527,\n",
              " 'evening': 528,\n",
              " 'visit': 529,\n",
              " 'tea': 530,\n",
              " 'perfect': 531,\n",
              " 'final': 532,\n",
              " 'songs': 533,\n",
              " 'dream': 534,\n",
              " 'can': 535,\n",
              " 'lil': 536,\n",
              " 'town': 537,\n",
              " 'listen': 538,\n",
              " 'studying': 539,\n",
              " 'meeting': 540,\n",
              " 'mothers': 541,\n",
              " 'nap': 542,\n",
              " 'weird': 543,\n",
              " 'seem': 544,\n",
              " 'sigh': 545,\n",
              " 'close': 546,\n",
              " 'fall': 547,\n",
              " 'loving': 548,\n",
              " 'tickets': 549,\n",
              " 'side': 550,\n",
              " 'dance': 551,\n",
              " 'gym': 552,\n",
              " 'less': 553,\n",
              " 'test': 554,\n",
              " 'when': 555,\n",
              " 'english': 556,\n",
              " 'hang': 557,\n",
              " 'mood': 558,\n",
              " 'ate': 559,\n",
              " 'interesting': 560,\n",
              " 'knew': 561,\n",
              " 'catch': 562,\n",
              " 'cat': 563,\n",
              " 'agree': 564,\n",
              " 'cream': 565,\n",
              " 'turn': 566,\n",
              " 'second': 567,\n",
              " 'clean': 568,\n",
              " 'list': 569,\n",
              " 'store': 570,\n",
              " 'moment': 571,\n",
              " 'worst': 572,\n",
              " 'writing': 573,\n",
              " 'story': 574,\n",
              " 'awards': 575,\n",
              " 'saying': 576,\n",
              " 'forget': 577,\n",
              " 'worth': 578,\n",
              " 'ahh': 579,\n",
              " 'word': 580,\n",
              " 'then': 581,\n",
              " 'ride': 582,\n",
              " 'supposed': 583,\n",
              " 'chocolate': 584,\n",
              " 'pool': 585,\n",
              " 'smile': 586,\n",
              " 'wishing': 587,\n",
              " 'london': 588,\n",
              " 'broken': 589,\n",
              " 'fast': 590,\n",
              " 'page': 591,\n",
              " 'via': 592,\n",
              " 'unfortunately': 593,\n",
              " 'only': 594,\n",
              " 'air': 595,\n",
              " 'past': 596,\n",
              " 'moving': 597,\n",
              " 'driving': 598,\n",
              " 'xxx': 599,\n",
              " 'account': 600,\n",
              " 'three': 601,\n",
              " 'throat': 602,\n",
              " 'sent': 603,\n",
              " 'pictures': 604,\n",
              " 'brothers': 605,\n",
              " 'gave': 606,\n",
              " 'yep': 607,\n",
              " 'dreams': 608,\n",
              " 'wedding': 609,\n",
              " 'short': 610,\n",
              " 'park': 611,\n",
              " 'understand': 612,\n",
              " 'photo': 613,\n",
              " 'cleaning': 614,\n",
              " 'followfriday': 615,\n",
              " 'horrible': 616,\n",
              " 'black': 617,\n",
              " 'sunshine': 618,\n",
              " 'sleepy': 619,\n",
              " 'whats': 620,\n",
              " 'drinking': 621,\n",
              " 'pick': 622,\n",
              " 'plan': 623,\n",
              " 'jonas': 624,\n",
              " 'tweeting': 625,\n",
              " 'star': 626,\n",
              " 'college': 627,\n",
              " 'chance': 628,\n",
              " 'wonder': 629,\n",
              " 'worse': 630,\n",
              " 'rather': 631,\n",
              " 'hugs': 632,\n",
              " 'longer': 633,\n",
              " 'slow': 634,\n",
              " 'fell': 635,\n",
              " 'team': 636,\n",
              " 'hmm': 637,\n",
              " 'vote': 638,\n",
              " 'sat': 639,\n",
              " 'york': 640,\n",
              " 'scared': 641,\n",
              " 'point': 642,\n",
              " 'bet': 643,\n",
              " 'easy': 644,\n",
              " 'apparently': 645,\n",
              " 'parents': 646,\n",
              " 'date': 647,\n",
              " 'with': 648,\n",
              " 'upset': 649,\n",
              " 'moon': 650,\n",
              " 'due': 651,\n",
              " 'flight': 652,\n",
              " 'spent': 653,\n",
              " 'green': 654,\n",
              " 'lady': 655,\n",
              " 'special': 656,\n",
              " 'mac': 657,\n",
              " 'huge': 658,\n",
              " 'holiday': 659,\n",
              " 'updates': 660,\n",
              " 'mum': 661,\n",
              " 'plans': 662,\n",
              " 'mtv': 663,\n",
              " 'hows': 664,\n",
              " 'spend': 665,\n",
              " 'where': 666,\n",
              " 'hand': 667,\n",
              " 'tuesday': 668,\n",
              " 'hanging': 669,\n",
              " 'do': 670,\n",
              " 'plus': 671,\n",
              " 'words': 672,\n",
              " 'flu': 673,\n",
              " 'fair': 674,\n",
              " 'nope': 675,\n",
              " 'earlier': 676,\n",
              " 'join': 677,\n",
              " 'wondering': 678,\n",
              " 'shows': 679,\n",
              " 'band': 680,\n",
              " 'sims': 681,\n",
              " 'shame': 682,\n",
              " 'website': 683,\n",
              " 'miley': 684,\n",
              " 'worry': 685,\n",
              " 'lazy': 686,\n",
              " 'body': 687,\n",
              " 'lets': 688,\n",
              " 'forever': 689,\n",
              " 'bus': 690,\n",
              " 'slept': 691,\n",
              " 'message': 692,\n",
              " 'wear': 693,\n",
              " 'answer': 694,\n",
              " 'mother': 695,\n",
              " 'thinks': 696,\n",
              " 'vacation': 697,\n",
              " 'white': 698,\n",
              " 'havent': 699,\n",
              " 'ahhh': 700,\n",
              " 'figure': 701,\n",
              " 'stomach': 702,\n",
              " 'warm': 703,\n",
              " 'beer': 704,\n",
              " 'looked': 705,\n",
              " 'learn': 706,\n",
              " 'thursday': 707,\n",
              " 'voice': 708,\n",
              " 'sadly': 709,\n",
              " 'july': 710,\n",
              " 'especially': 711,\n",
              " 'different': 712,\n",
              " 'support': 713,\n",
              " 'fans': 714,\n",
              " 'possible': 715,\n",
              " 'die': 716,\n",
              " 'cake': 717,\n",
              " 'meant': 718,\n",
              " 'line': 719,\n",
              " 'inside': 720,\n",
              " 'safe': 721,\n",
              " 'chat': 722,\n",
              " 'met': 723,\n",
              " 'application': 724,\n",
              " 'number': 725,\n",
              " 'photos': 726,\n",
              " 'liked': 727,\n",
              " 'myspace': 728,\n",
              " 'officially': 729,\n",
              " 'google': 730,\n",
              " 'episode': 731,\n",
              " 'mins': 732,\n",
              " 'fix': 733,\n",
              " 'rainy': 734,\n",
              " 'david': 735,\n",
              " 'crying': 736,\n",
              " 'camera': 737,\n",
              " 'dress': 738,\n",
              " 'airport': 739,\n",
              " 'small': 740,\n",
              " 'absolutely': 741,\n",
              " 'pizza': 742,\n",
              " 'bbq': 743,\n",
              " 'yummy': 744,\n",
              " 'tom': 745,\n",
              " 'who': 746,\n",
              " 'shop': 747,\n",
              " 'games': 748,\n",
              " 'tummy': 749,\n",
              " 'worked': 750,\n",
              " 'shall': 751,\n",
              " 'luv': 752,\n",
              " 'paper': 753,\n",
              " 'felt': 754,\n",
              " 'decided': 755,\n",
              " 'proud': 756,\n",
              " 'rip': 757,\n",
              " 'boyfriend': 758,\n",
              " 'radio': 759,\n",
              " 'graduation': 760,\n",
              " 'power': 761,\n",
              " 'project': 762,\n",
              " 'finals': 763,\n",
              " 'save': 764,\n",
              " 'garden': 765,\n",
              " 'except': 766,\n",
              " 'shoes': 767,\n",
              " 'needed': 768,\n",
              " 'beat': 769,\n",
              " 'eye': 770,\n",
              " 'kill': 771,\n",
              " 'bike': 772,\n",
              " 'played': 773,\n",
              " 'wit': 774,\n",
              " 'road': 775,\n",
              " 'hug': 776,\n",
              " 'has': 777,\n",
              " 'gorgeous': 778,\n",
              " 'nights': 779,\n",
              " 'lonely': 780,\n",
              " 'starts': 781,\n",
              " 'annoying': 782,\n",
              " 'keeps': 783,\n",
              " 'blue': 784,\n",
              " 'books': 785,\n",
              " 'chicken': 786,\n",
              " 'exactly': 787,\n",
              " 'apple': 788,\n",
              " 'hospital': 789,\n",
              " 'been': 790,\n",
              " 'alright': 791,\n",
              " 'isnt': 792,\n",
              " 'wishes': 793,\n",
              " 'case': 794,\n",
              " 'exciting': 795,\n",
              " 'kid': 796,\n",
              " 'sign': 797,\n",
              " 'cos': 798,\n",
              " 'yup': 799,\n",
              " 'having': 800,\n",
              " 'hates': 801,\n",
              " 'yall': 802,\n",
              " 'front': 803,\n",
              " 'card': 804,\n",
              " 'twilight': 805,\n",
              " 'taken': 806,\n",
              " 'feet': 807,\n",
              " 'son': 808,\n",
              " 'more': 809,\n",
              " 'french': 810,\n",
              " 'company': 811,\n",
              " 'wine': 812,\n",
              " 'living': 813,\n",
              " 'xoxo': 814,\n",
              " 'fact': 815,\n",
              " 'wednesday': 816,\n",
              " 'near': 817,\n",
              " 'woo': 818,\n",
              " 'babe': 819,\n",
              " 'lame': 820,\n",
              " 'packing': 821,\n",
              " 'turned': 822,\n",
              " 'whatever': 823,\n",
              " 'goodbye': 824,\n",
              " 'scary': 825,\n",
              " 'knows': 826,\n",
              " 'hubby': 827,\n",
              " 'realized': 828,\n",
              " 'share': 829,\n",
              " 'service': 830,\n",
              " 'once': 831,\n",
              " 'club': 832,\n",
              " 'happens': 833,\n",
              " 'couldnt': 834,\n",
              " 'behind': 835,\n",
              " 'ouch': 836,\n",
              " 'question': 837,\n",
              " 'sold': 838,\n",
              " 'download': 839,\n",
              " 'jus': 840,\n",
              " 'here': 841,\n",
              " 'waking': 842,\n",
              " 'gettin': 843,\n",
              " 'pass': 844,\n",
              " 'business': 845,\n",
              " 'cup': 846,\n",
              " 'giving': 847,\n",
              " 'killing': 848,\n",
              " 'videos': 849,\n",
              " 'minute': 850,\n",
              " 'lose': 851,\n",
              " 'drunk': 852,\n",
              " 'walking': 853,\n",
              " 'clothes': 854,\n",
              " 'order': 855,\n",
              " 'although': 856,\n",
              " 'along': 857,\n",
              " 'enjoyed': 858,\n",
              " 'relaxing': 859,\n",
              " 'film': 860,\n",
              " 'terrible': 861,\n",
              " 'bro': 862,\n",
              " 'everybody': 863,\n",
              " 'alot': 864,\n",
              " 'hahaha': 865,\n",
              " 'touch': 866,\n",
              " 'passed': 867,\n",
              " 'guitar': 868,\n",
              " 'fantastic': 869,\n",
              " 'posted': 870,\n",
              " 'sis': 871,\n",
              " 'vegas': 872,\n",
              " 'random': 873,\n",
              " 'revision': 874,\n",
              " 'some': 875,\n",
              " 'sit': 876,\n",
              " 'asked': 877,\n",
              " 'version': 878,\n",
              " 'aint': 879,\n",
              " 'staying': 880,\n",
              " 'round': 881,\n",
              " 'ahead': 882,\n",
              " 'interview': 883,\n",
              " 'single': 884,\n",
              " 'indeed': 885,\n",
              " 'on': 886,\n",
              " 'box': 887,\n",
              " 'hmmm': 888,\n",
              " 'singing': 889,\n",
              " 'light': 890,\n",
              " 'deal': 891,\n",
              " 'bummed': 892,\n",
              " 'vip': 893,\n",
              " 'lakers': 894,\n",
              " 'fly': 895,\n",
              " 'completely': 896,\n",
              " 'history': 897,\n",
              " 'upload': 898,\n",
              " 'web': 899,\n",
              " 'comment': 900,\n",
              " 'dark': 901,\n",
              " 'dying': 902,\n",
              " 'ohh': 903,\n",
              " 'as': 904,\n",
              " 'wife': 905,\n",
              " 'currently': 906,\n",
              " 'lately': 907,\n",
              " 'bitch': 908,\n",
              " 'usually': 909,\n",
              " 'changed': 910,\n",
              " 'freaking': 911,\n",
              " 'shirt': 912,\n",
              " 'others': 913,\n",
              " 'plane': 914,\n",
              " 'hun': 915,\n",
              " 'wearing': 916,\n",
              " 'huh': 917,\n",
              " 'min': 918,\n",
              " 'headed': 919,\n",
              " 'disappointed': 920,\n",
              " 'bloody': 921,\n",
              " 'hangover': 922,\n",
              " 'awful': 923,\n",
              " 'worried': 924,\n",
              " 'gosh': 925,\n",
              " 'fingers': 926,\n",
              " 'fml': 927,\n",
              " 'shoot': 928,\n",
              " 'camp': 929,\n",
              " 'ran': 930,\n",
              " 'traffic': 931,\n",
              " 'peace': 932,\n",
              " 'watchin': 933,\n",
              " 'again': 934,\n",
              " 'hotel': 935,\n",
              " 'sing': 936,\n",
              " 'hold': 937,\n",
              " 'quick': 938,\n",
              " 'caught': 939,\n",
              " 'spending': 940,\n",
              " 'death': 941,\n",
              " 'bar': 942,\n",
              " 'extra': 943,\n",
              " 'serious': 944,\n",
              " 'country': 945,\n",
              " 'nearly': 946,\n",
              " 'fathers': 947,\n",
              " 'itunes': 948,\n",
              " 'dogs': 949,\n",
              " 'sexy': 950,\n",
              " 'somewhere': 951,\n",
              " 'fat': 952,\n",
              " 'wasnt': 953,\n",
              " 'coz': 954,\n",
              " 'art': 955,\n",
              " 'exhausted': 956,\n",
              " 'closed': 957,\n",
              " 'appreciate': 958,\n",
              " 'pissed': 959,\n",
              " 'door': 960,\n",
              " 'matter': 961,\n",
              " 'fixed': 962,\n",
              " 'profile': 963,\n",
              " 'cheese': 964,\n",
              " 'yum': 965,\n",
              " 'maths': 966,\n",
              " 'nose': 967,\n",
              " 'chillin': 968,\n",
              " 'after': 969,\n",
              " 'should': 970,\n",
              " 'cook': 971,\n",
              " 'ooh': 972,\n",
              " 'problems': 973,\n",
              " 'blood': 974,\n",
              " 'takes': 975,\n",
              " 'dunno': 976,\n",
              " 'math': 977,\n",
              " 'dvd': 978,\n",
              " 'pink': 979,\n",
              " 'def': 980,\n",
              " 'ended': 981,\n",
              " 'mommy': 982,\n",
              " 'hands': 983,\n",
              " 'stopped': 984,\n",
              " 'theres': 985,\n",
              " 'dang': 986,\n",
              " 'silly': 987,\n",
              " 'mall': 988,\n",
              " 'gay': 989,\n",
              " 'awwww': 990,\n",
              " 'any': 991,\n",
              " 'ticket': 992,\n",
              " 'sooooo': 993,\n",
              " 'miles': 994,\n",
              " 'daughter': 995,\n",
              " 'group': 996,\n",
              " 'info': 997,\n",
              " 'windows': 998,\n",
              " 'nobody': 999,\n",
              " 'shot': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYStmQX3iynS"
      },
      "source": [
        "Création de vecteurs \"one-hot\" à partir de tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1dDS_vIi0Ef"
      },
      "source": [
        "# Your code\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "sequences = pad_sequences(sequences, padding='post', value='<pad>', dtype=object, maxlen=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnb7RbSOi5RK"
      },
      "source": [
        "Création d'ensembles d'entraînement pour le réseau neuronal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRAmyFgKi0dF"
      },
      "source": [
        "data = tokenizer.sequences_to_matrix(sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialisation du modèle"
      ],
      "metadata": {
        "id": "Byyks4xiQUzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En utilisant le séquentiel ([doc](https://keras.io/guides/sequential_model/)), initialiser un modèle d'apprentissage profond avec :\n",
        "- une couche d'imbrication ([doc](https://keras.io/api/layers/core_layers/embedding/)) (input_dim=vocab_size+1, output_dim=embedding_dim, input_length=max_length)\n",
        "- LSTM ([doc](https://keras.io/api/layers/recurrent_layers/lstm/)) (unités=64)\n",
        "- Dense ([doc](https://keras.io/api/layers/core_layers/dense/)) (unités=1)"
      ],
      "metadata": {
        "id": "VevWvDrvQd1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code\n",
        "model = Sequential()\n",
        "model.add(tf.keras.layers.Embedding(tokenizer.word_index.size+1, embedding_dim, input_length=max_length))\n",
        "model.add(tf.keras.layers.LSTM(64))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "VMhgAZpQQTZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilez votre modèle en utilisant la méthode ``.compile`` [doc](https://keras.io/api/models/model/) en utilisant les paramètres (loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "zBUMDNVnQiva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "leQI2xwxQjLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entraînez votre modèle "
      ],
      "metadata": {
        "id": "frzUWtXXQmTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "\n",
        "training_padded = None\n",
        "training_labels = None\n",
        "testing_padded = None\n",
        "testing_labels = None"
      ],
      "metadata": {
        "id": "LW53m3BbQqgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = None\n",
        "  \n",
        "print(\"Training Complete\")"
      ],
      "metadata": {
        "id": "-1Lpe_5jQm2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisation des résultats"
      ],
      "metadata": {
        "id": "LsSdy9Z9QwX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc=history.history['accuracy']\n",
        "val_acc=history.history['val_accuracy']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, acc, 'r')\n",
        "plt.plot(epochs, val_acc, 'b')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n",
        "\n",
        "plt.figure()\n",
        "plt.show()\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, loss, 'r')\n",
        "plt.plot(epochs, val_loss, 'b')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"Loss\", \"Validation Loss\"])\n",
        "\n",
        "plt.figure()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "psgxORz5Qy0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7YXyed3i89q"
      },
      "source": [
        "# GLoVe : Création de la matrice d'embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca1vCbyIi6dS"
      },
      "source": [
        "embeddings_index = {};\n",
        "\n",
        "with open('/content/drive/MyDrive/Twitter_Dataset/glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split();\n",
        "        word = values[0];\n",
        "        coefs = np.asarray(values[1:], dtype='float32');\n",
        "        embeddings_index[word] = coefs;\n",
        "\n",
        "embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word);\n",
        "    if embedding_vector is not None:\n",
        "        embeddings_matrix[i] = embedding_vector;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5xyOPvAjkoa"
      },
      "source": [
        "#### Création du réseau neuronal LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxPRHTxEj9YT"
      },
      "source": [
        "En utilisant le séquentiel ([doc](https://keras.io/guides/sequential_model/)), initialiser un modèle d'apprentissage profond avec :\n",
        "- une couche d'imbrication ([doc](https://keras.io/api/layers/core_layers/embedding/)) (input_dim=vocab_size+1, output_dim=embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False)\n",
        "- LSTM ([doc](https://keras.io/api/layers/recurrent_layers/lstm/)) (unités=64)\n",
        "- Dense ([doc](https://keras.io/api/layers/core_layers/dense/)) (unités=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq08ixKyjlz6"
      },
      "source": [
        "# Your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVdzD9qGlfs9"
      },
      "source": [
        "Compilez votre modèle en utilisant la méthode ``.compile`` [doc](https://keras.io/api/models/model/) en utilisant les paramètres (loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NgQC3RUjpdH"
      },
      "source": [
        "# Your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7IIh9WmmEm3"
      },
      "source": [
        "Utilisez la méthode ``.summary()`` à votre modèle pour le visualiser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIcrynnBlVfn"
      },
      "source": [
        "# Your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q69bcT6XmQKo"
      },
      "source": [
        "# Entraînez votre modèle "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thfpkI-GmNGv"
      },
      "source": [
        "num_epochs = 10\n",
        "\n",
        "training_padded = None\n",
        "training_labels = None\n",
        "testing_padded = None\n",
        "testing_labels = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ13BPTenWH3"
      },
      "source": [
        "Utilisez la méthode ``.fit`` pour entraîner votre modèle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0Pw6kCzmTxq"
      },
      "source": [
        "history = None\n",
        "  \n",
        "print(\"Training Complete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2hFtuUdmYXz"
      },
      "source": [
        "Sauvegardez votre modèle en utilisant la méthode ``.save()`` en utilisant comme paramètre le path='/content/drive/MyDrive/Twitter_Dataset/model_train.h5'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOe0iPREmVEb"
      },
      "source": [
        "# Your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_mAU-5Hmuj1"
      },
      "source": [
        "# Visualize your training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "797qiwl5mtuj"
      },
      "source": [
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc=history.history['accuracy']\n",
        "val_acc=history.history['val_accuracy']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, acc, 'r')\n",
        "plt.plot(epochs, val_acc, 'b')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n",
        "\n",
        "plt.figure()\n",
        "plt.show()\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, loss, 'r')\n",
        "plt.plot(epochs, val_loss, 'b')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"Loss\", \"Validation Loss\"])\n",
        "\n",
        "plt.figure()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}